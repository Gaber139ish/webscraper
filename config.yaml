start_urls:
  - "https://news.ycombinator.com"
  - "https://arxiv.org"
max_depth: 2
concurrency: 2
user_agent: "Mozilla/5.0 (X11; Linux x86_64) CoineyScraper/1.0"
headless: true
# proxy: { server: "http://host:port", username: "user", password: "pass" }
output:
  jsonl: "./exports/dataset.jsonl"
  sqlite: "./exports/dataset.db"
  snapshots_dir: "./exports/snapshots"
crawl:
  follow_external: false
  respect_robots: true
  wait_after_load: 1.0
  intercept_api: true
  max_retries: 2
  backoff_base: 0.75
  allow_domains: []
  deny_domains: []
  deny_extensions: [".jpg", ".jpeg", ".png", ".gif", ".svg", ".webp", ".ico", ".pdf", ".zip", ".gz", ".tar", ".rar", ".7z", ".mp3", ".mp4"]
  save_html_snapshot: false
  save_screenshot: false
rate_limit:
  delay_seconds: 0.5
  per_domain_delay_seconds: 0.0
  per_domain_concurrency: 0
deep_crawl:
  infinite_scroll:
    enabled: false
    max_iterations: 8
    wait_seconds: 0.8
  click_more_selectors: []  # e.g. ["button.load-more", ".pagination .next a"]
  max_clicks: 10
  click_wait_seconds: 0.8
  forms: []
  # Example form:
  # - url: "https://example.com/search"
  #   queries: ["ai", "python"]
  #   # or: queries_file: "queries.txt"
  #   fields: {"input[name=q]": "{query}"}
  #   submit_selector: "form button[type=submit]"
  #   wait_after_submit: 1.5
  #   max_results_per_query: 20
github:
  token: ""                # optional, recommended for higher rate limits
  query: "language:python machine learning"
  per_page: 10
  pages: 2
  max_files_per_repo: 200
  extensions: [".py", ".js", ".cpp", ".c", ".java", ".rs"]
  max_file_size: 200000   # bytes
  concurrency: 8
  output_dir: "exports/github_code"
